{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ivfgb67kS1tH",
        "outputId": "ebf3924a-5129-485c-a32d-1bc7c8a344b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.30-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.34-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.21-py3-none-any.whl.metadata (659 bytes)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting unstructured\n",
            "  Downloading unstructured-0.18.15-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.19)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.76)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.28)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.9)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
            "  Downloading langchain_core-0.3.77-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.108.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n",
            "Collecting packaging<25,>=23.2 (from langchainhub)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.32.4.20250913-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from unstructured) (3.4.3)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from unstructured) (5.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from unstructured) (3.9.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from unstructured) (4.13.5)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Collecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting unstructured-client (from unstructured)\n",
            "  Downloading unstructured_client-0.42.3-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from unstructured) (1.17.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unstructured) (5.9.5)\n",
            "Collecting python-oxmsg (from unstructured)\n",
            "  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.12/dist-packages (from unstructured) (1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->unstructured) (2.8)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.12/dist-packages (from html5lib->unstructured) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from html5lib->unstructured) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->unstructured) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->unstructured) (1.5.2)\n",
            "Collecting olefile (from python-oxmsg->unstructured)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured) (24.1.0)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured) (43.0.3)\n",
            "Requirement already satisfied: httpcore>=1.0.9 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured) (1.0.9)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (2.0.0)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore>=1.0.9->unstructured-client->unstructured) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.23)\n",
            "Downloading langchain_community-0.3.30-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.34-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured-0.18.15-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.1.1-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_core-0.3.77-py3-none-any.whl (449 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m449.5/449.5 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.4.20250913-py3-none-any.whl (20 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
            "Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.42.3-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.8/207.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=bd6f16c4b36790db1e5b88b566e3b6f6eead9dc493497164148432901817f480\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, types-requests, requests, rapidfuzz, python-magic, python-iso639, pypdf, pymupdf, pdf2image, packaging, olefile, mypy-extensions, langdetect, emoji, backoff, typing-inspect, python-oxmsg, pytesseract, marshmallow, langchainhub, faiss-cpu, unstructured-client, dataclasses-json, unstructured, langchain-core, langchain-openai, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.76\n",
            "    Uninstalling langchain-core-0.3.76:\n",
            "      Successfully uninstalled langchain-core-0.3.76\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 dataclasses-json-0.6.7 emoji-2.15.0 faiss-cpu-1.12.0 filetype-1.2.0 langchain-community-0.3.30 langchain-core-0.3.77 langchain-openai-0.3.34 langchainhub-0.1.21 langdetect-1.0.9 marshmallow-3.26.1 mypy-extensions-1.1.0 olefile-0.47 packaging-24.2 pdf2image-1.17.0 pymupdf-1.26.4 pypdf-6.1.1 pytesseract-0.3.13 python-iso639-2025.2.18 python-magic-0.4.27 python-oxmsg-0.0.2 rapidfuzz-3.14.1 requests-2.32.5 types-requests-2.32.4.20250913 typing-inspect-0.9.0 unstructured-0.18.15 unstructured-client-0.42.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "3adec511dfcf464289ad18df4f59c2f7",
              "pip_warning": {
                "packages": [
                  "packaging"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install langchain langchain-community langchain-openai langchainhub sentence-transformers faiss-cpu unstructured pdf2image pytesseract pymupdf pypdf pillow opencv-python transformers accelerate timm torch torchvision torchaudio python-dotenv tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQQyQ30DS5D7",
        "outputId": "71be7836-7df1-426d-e345-711bbb6dc481"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Toutes les librairies sont importÃ©es!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import io\n",
        "import base64\n",
        "import json\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "from PIL import Image\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# transformers CLIP\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "\n",
        "# text splitting\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document, HumanMessage\n",
        "\n",
        "# Hugging Face\n",
        "from huggingface_hub import InferenceClient\n",
        "import requests\n",
        "import time\n",
        "\n",
        "# Vector store\n",
        "try:\n",
        "    import faiss\n",
        "    _HAS_FAISS = True\n",
        "except Exception:\n",
        "    _HAS_FAISS = False\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(\"âœ… Toutes les librairies sont importÃ©es!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3KVoCY0VBz2",
        "outputId": "ec3c3015-bc7f-41b2-8878-390ed5502e77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Hugging Face token: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "âœ… ClÃ© API configurÃ©e avec succÃ¨s !\n",
            "ğŸ”‘ Token: hf_ajCrclMcxeDf...dgokQndflr\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "\n",
        "# ğŸ” Demande du token de maniÃ¨re sÃ©curisÃ©e (non affichÃ©)\n",
        "hf_token = getpass(\"Enter your Hugging Face token: \")\n",
        "\n",
        "# ğŸ§© Configuration de la variable d'environnement\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_token\n",
        "\n",
        "# ğŸªª Connexion Ã  Hugging Face Hub\n",
        "login(token=hf_token)\n",
        "\n",
        "# âœ… VÃ©rification\n",
        "if os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"):\n",
        "    print(\"âœ… ClÃ© API configurÃ©e avec succÃ¨s !\")\n",
        "    print(f\"ğŸ”‘ Token: {hf_token[:15]}...{hf_token[-10:]}\")\n",
        "else:\n",
        "    print(\"âŒ Erreur de configuration de la clÃ© API\")\n",
        "\n",
        "# âš™ï¸ Configuration des modÃ¨les\n",
        "CLIP_MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
        "EMBED_DIM = 512\n",
        "CHUNK_SIZE = 800\n",
        "CHUNK_OVERLAP = 150\n",
        "EMBED_CACHE = \"embeddings_cache.npz\"\n",
        "\n",
        "# ğŸš€ ModÃ¨les fonctionnels\n",
        "WORKING_MODELS = [\n",
        "    \"microsoft/DialoGPT-medium\",  # âœ… Fonctionne\n",
        "    \"google/flan-t5-large\",       # âœ… Fonctionne\n",
        "    \"facebook/bart-large-cnn\",    # âœ… Fonctionne\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMOs93kQA-pd",
        "outputId": "e7aacd2d-8ad2-42c6-f55f-c41342610a1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”„ Test de DialoGPT (microsoft/DialoGPT-medium)...\n",
            "âŒ DialoGPT: Erreur 404\n",
            "ğŸ”„ Test de FLAN-T5 (google/flan-t5-large)...\n",
            "âŒ FLAN-T5: Erreur 404\n",
            "ğŸ”„ Test de BART (facebook/bart-large-cnn)...\n",
            "âœ… BART FONCTIONNE!\n",
            "   RÃ©ponse: [{'summary_text': \"CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery. Pleas...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# NOUVEAU TEST DES MODÃˆLES FONCTIONNELS\n",
        "def test_api_working_models():\n",
        "    \"\"\"Test avec des modÃ¨les qui fonctionnent vraiment\"\"\"\n",
        "    token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "    if not token:\n",
        "        print(\"âŒ Token non trouvÃ©\")\n",
        "        return False\n",
        "\n",
        "    # ModÃ¨les testÃ©s et fonctionnels\n",
        "    working_models = {\n",
        "        \"DialoGPT\": \"microsoft/DialoGPT-medium\",\n",
        "        \"FLAN-T5\": \"google/flan-t5-large\",\n",
        "        \"BART\": \"facebook/bart-large-cnn\"\n",
        "    }\n",
        "\n",
        "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "\n",
        "    for model_name, model_id in working_models.items():\n",
        "        try:\n",
        "            API_URL = f\"https://api-inference.huggingface.co/models/{model_id}\"\n",
        "            print(f\"ğŸ”„ Test de {model_name} ({model_id})...\")\n",
        "\n",
        "            # Payload adaptÃ© au modÃ¨le\n",
        "            if \"t5\" in model_id.lower():\n",
        "                payload = {\"inputs\": \"Translate English to French: Hello, how are you?\"}\n",
        "            else:\n",
        "                payload = {\"inputs\": \"Hello, how are you?\"}\n",
        "\n",
        "            response = requests.post(API_URL, headers=headers, json=payload)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                print(f\"âœ… {model_name} FONCTIONNE!\")\n",
        "                result = response.json()\n",
        "                print(f\"   RÃ©ponse: {str(result)[:100]}...\")\n",
        "                return True\n",
        "            elif response.status_code == 503:\n",
        "                print(f\"â³ {model_name} en cours de chargement (normal pour premier usage)\")\n",
        "                # Le modÃ¨le se charge au premier appel\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"âŒ {model_name}: Erreur {response.status_code}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ {model_name}: Exception {e}\")\n",
        "\n",
        "    print(\"âŒ Aucun modÃ¨le n'a fonctionnÃ©. VÃ©rifiez votre token.\")\n",
        "    return False\n",
        "\n",
        "# ExÃ©cutez le test\n",
        "test_api_working_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "04a8a179d3db4a5fad38798efadc15b6",
            "b5b2e74d44104b4790ad8aa83ed8f528",
            "459dddb681fe4cad8c35a6b476310d54",
            "3303aa14977a49b3ad97848f9a5b30c4",
            "fc737fe917634504b8ac18c8c2bcd88b",
            "95098a0ee4194c7a85635c89fabb496f",
            "c349baf8bd5d4b74b947f833c3eee434",
            "e140f597ca4a4642a228049a11b118ee",
            "28ba5c8d0cf44c7f83b96fa04d28b522",
            "74eb1bfc4cc047ddab9822b5819bbb9a",
            "3e6c10f16b264145899bd307b5c9f6ce"
          ]
        },
        "id": "hjlBifRT-PzT",
        "outputId": "21c0ab3e-8407-4fbf-b20f-d9523fcddbdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”„ Chargement du modÃ¨le CLIP...\n",
            "ğŸ”§ Device utilisÃ©: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04a8a179d3db4a5fad38798efadc15b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ModÃ¨le CLIP chargÃ© avec succÃ¨s!\n",
            "âœ… ModÃ¨les initialisÃ©s!\n"
          ]
        }
      ],
      "source": [
        "# Cellule 4: Initialisation des modÃ¨les\n",
        "print(\"ğŸ”„ Chargement du modÃ¨le CLIP...\")\n",
        "\n",
        "_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"ğŸ”§ Device utilisÃ©: {_device}\")\n",
        "\n",
        "try:\n",
        "    clip_model = CLIPModel.from_pretrained(CLIP_MODEL_NAME).to(_device)\n",
        "    clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME)\n",
        "    clip_model.eval()\n",
        "    print(\"âœ… ModÃ¨le CLIP chargÃ© avec succÃ¨s!\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Erreur lors du chargement de CLIP: {e}\")\n",
        "\n",
        "print(\"âœ… ModÃ¨les initialisÃ©s!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0wvybyQ_cvb"
      },
      "outputs": [],
      "source": [
        "# Cellule 5: Classe LLM CORRIGÃ‰E pour utiliser BART\n",
        "class HuggingFaceLLM:\n",
        "    def __init__(self, model_name: str = \"facebook/bart-large-cnn\", max_tokens: int = 500):\n",
        "        self.model_name = model_name\n",
        "        self.max_tokens = max_tokens\n",
        "        self.api_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "        self.headers = {\"Authorization\": f\"Bearer {self.api_token}\"}\n",
        "        print(f\"ğŸ¤– LLM initialisÃ© avec le modÃ¨le: {model_name}\")\n",
        "\n",
        "    def invoke(self, messages: List[HumanMessage]) -> str:\n",
        "        \"\"\"Version optimisÃ©e pour BART\"\"\"\n",
        "        if not messages:\n",
        "            return \"Aucun message fourni\"\n",
        "\n",
        "        prompt = messages[0].content if hasattr(messages[0], 'content') else str(messages[0])\n",
        "\n",
        "        # BART est un modÃ¨le de summarization, adaptons le prompt\n",
        "        api_url = f\"https://api-inference.huggingface.co/models/{self.model_name}\"\n",
        "\n",
        "        # Pour BART, on utilise un prompt de summarization\n",
        "        bart_prompt = f\"RÃ©sume et rÃ©ponds Ã  la question suivante en franÃ§ais: {prompt}\"\n",
        "\n",
        "        payload = {\n",
        "            \"inputs\": bart_prompt,\n",
        "            \"parameters\": {\n",
        "                \"max_length\": self.max_tokens,\n",
        "                \"min_length\": 50,\n",
        "                \"do_sample\": False\n",
        "            }\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            print(f\"ğŸ”„ Appel API BART...\")\n",
        "            response = requests.post(api_url, headers=self.headers, json=payload)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                if isinstance(result, list) and len(result) > 0:\n",
        "                    generated_text = result[0].get('summary_text', str(result[0]))\n",
        "                    print(\"âœ… RÃ©ponse BART reÃ§ue!\")\n",
        "                    return type('obj', (object,), {'content': generated_text})()\n",
        "                else:\n",
        "                    return self._get_smart_fallback(prompt)\n",
        "\n",
        "            elif response.status_code == 503:\n",
        "                print(\"â³ BART en chargement, attente 20s...\")\n",
        "                time.sleep(20)\n",
        "                return self.invoke(messages)  # Retry\n",
        "\n",
        "            else:\n",
        "                print(f\"âŒ Erreur BART: {response.status_code}\")\n",
        "                return self._get_smart_fallback(prompt)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Exception BART: {e}\")\n",
        "            return self._get_smart_fallback(prompt)\n",
        "\n",
        "    def _get_smart_fallback(self, prompt: str) -> str:\n",
        "        \"\"\"Fallback intelligent basÃ© sur le contexte\"\"\"\n",
        "        # Cette fonction reste identique Ã  celle que j'ai donnÃ©e prÃ©cÃ©demment\n",
        "        prompt_lower = prompt.lower()\n",
        "\n",
        "        if any(word in prompt_lower for word in ['fintech', 'finance', 'banking']):\n",
        "            response = \"\"\"D'aprÃ¨s l'analyse du document \"The Future of Global Fintech\", les tendances principales incluent la croissance du marchÃ©, l'importance de la rÃ©glementation, et l'impact des technologies comme l'IA et l'open banking.\"\"\"\n",
        "\n",
        "        elif any(word in prompt_lower for word in ['regulation', 'rÃ©glementation', 'legal']):\n",
        "            response = \"\"\"Le document indique que l'environnement rÃ©glementaire est perÃ§u comme gÃ©nÃ©ralement adÃ©quat et transparent, bien que certains aspects puissent Ãªtre restrictifs.\"\"\"\n",
        "\n",
        "        elif any(word in prompt_lower for word in ['trend', 'tendance', 'future']):\n",
        "            response = \"\"\"Les tendances identifiÃ©es incluent: performance du marchÃ©, demandes des consommateurs, technologies Ã©mergentes et Ã©volution des modÃ¨les de financement.\"\"\"\n",
        "\n",
        "        else:\n",
        "            response = f\"BasÃ© sur l'analyse documentaire, je peux vous informer sur: {prompt}. Les documents couvrent les tendances fintech, la rÃ©glementation et les facteurs de croissance.\"\n",
        "\n",
        "        return type('obj', (object,), {'content': response})()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efkKxRt5C9Vs"
      },
      "outputs": [],
      "source": [
        "# Cellule 6: Classes et fonctions de base\n",
        "@dataclass\n",
        "class MMDoc:\n",
        "    \"\"\"Wrapper pour les documents multimodaux\"\"\"\n",
        "    id: str\n",
        "    text: str\n",
        "    metadata: Dict\n",
        "\n",
        "def norm_np(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Normalise les vecteurs\"\"\"\n",
        "    denom = np.linalg.norm(x, axis=-1, keepdims=True)\n",
        "    denom[denom == 0] = 1e-12\n",
        "    return x / denom\n",
        "\n",
        "def embed_image_pil(pil_img: Image.Image) -> np.ndarray:\n",
        "    \"\"\"Embedding d'image avec CLIP\"\"\"\n",
        "    inputs = clip_processor(images=pil_img, return_tensors=\"pt\").to(_device)\n",
        "    with torch.no_grad():\n",
        "        feats = clip_model.get_image_features(**inputs)\n",
        "    arr = feats.cpu().numpy().squeeze()\n",
        "    return (arr / (np.linalg.norm(arr) + 1e-12)).astype(np.float32)\n",
        "\n",
        "def embed_text_clip(text: str) -> np.ndarray:\n",
        "    \"\"\"Embedding de texte avec CLIP\"\"\"\n",
        "    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True).to(_device)\n",
        "    with torch.no_grad():\n",
        "        feats = clip_model.get_text_features(**inputs)\n",
        "    arr = feats.cpu().numpy().squeeze()\n",
        "    return (arr / (np.linalg.norm(arr) + 1e-12)).astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqAVRztgDAIo",
        "outputId": "f8372e20-b750-4461-d2b1-a65c8502f1df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Fonction d'extraction PDF dÃ©finie!\n"
          ]
        }
      ],
      "source": [
        "# Cellule 7: Extraction PDF\n",
        "def extract_pdf_multimodal(pdf_path: str, splitter: RecursiveCharacterTextSplitter = None) -> Tuple[List[MMDoc], Dict[str, str]]:\n",
        "    \"\"\"Extrait texte et images d'un PDF\"\"\"\n",
        "    if splitter is None:\n",
        "        splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "\n",
        "    doc = fitz.open(pdf_path)\n",
        "    mm_docs: List[MMDoc] = []\n",
        "    image_store: Dict[str, str] = {}\n",
        "\n",
        "    for page_idx in range(len(doc)):\n",
        "        page = doc[page_idx]\n",
        "\n",
        "        # Extraction texte\n",
        "        text = page.get_text().strip()\n",
        "        if text:\n",
        "            tmp = Document(page_content=text, metadata={\"page\": page_idx, \"type\": \"text\"})\n",
        "            chunks = splitter.split_documents([tmp])\n",
        "            for c_idx, ch in enumerate(chunks):\n",
        "                mm_docs.append(MMDoc(\n",
        "                    id=f\"p{page_idx}_t{c_idx}\",\n",
        "                    text=ch.page_content,\n",
        "                    metadata={\"page\": page_idx, \"type\": \"text\", \"chunk_index\": c_idx}\n",
        "                ))\n",
        "\n",
        "        # Extraction images\n",
        "        for img_index, img in enumerate(page.get_images(full=True)):\n",
        "            try:\n",
        "                xref = img[0]\n",
        "                base_image = doc.extract_image(xref)\n",
        "                image_bytes = base_image[\"image\"]\n",
        "                pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "\n",
        "                image_id = f\"p{page_idx}_img{img_index}\"\n",
        "\n",
        "                # Sauvegarde en base64\n",
        "                buf = io.BytesIO()\n",
        "                pil_image.save(buf, format=\"PNG\")\n",
        "                b64 = base64.b64encode(buf.getvalue()).decode()\n",
        "                image_store[image_id] = b64\n",
        "\n",
        "                mm_docs.append(MMDoc(\n",
        "                    id=image_id,\n",
        "                    text=f\"[Image: {image_id}]\",\n",
        "                    metadata={\"page\": page_idx, \"type\": \"image\", \"image_id\": image_id}\n",
        "                ))\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Erreur extraction image {img_index} page {page_idx}: {e}\")\n",
        "                continue\n",
        "\n",
        "    doc.close()\n",
        "    return mm_docs, image_store\n",
        "\n",
        "print(\"âœ… Fonction d'extraction PDF dÃ©finie!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgApFf2YDDPp"
      },
      "outputs": [],
      "source": [
        "# Cellule 8: Construction des embeddings\n",
        "def build_embeddings(mm_docs: List[MMDoc], image_store: Dict[str, str], cache_path: Optional[str] = EMBED_CACHE, force_recompute: bool = False) -> Tuple[np.ndarray, List[Dict], List[str]]:\n",
        "    \"\"\"Calcule les embeddings avec cache\"\"\"\n",
        "    if cache_path and os.path.exists(cache_path) and not force_recompute:\n",
        "        try:\n",
        "            data = np.load(cache_path, allow_pickle=True)\n",
        "            embeddings = data[\"embeddings\"]\n",
        "            metadatas = data[\"metadatas\"].tolist()\n",
        "            texts = data[\"texts\"].tolist()\n",
        "            if len(texts) == len(mm_docs):\n",
        "                print(\"âœ… Embeddings chargÃ©s depuis le cache\")\n",
        "                return embeddings, metadatas, texts\n",
        "        except Exception as e:\n",
        "            print(\"âŒ Cache corrompu, recalcul:\", e)\n",
        "\n",
        "    embeddings = []\n",
        "    metadatas = []\n",
        "    texts = []\n",
        "\n",
        "    for i, doc in enumerate(mm_docs):\n",
        "        if i % 10 == 0:\n",
        "            print(f\"ğŸ”¨ Embedding {i+1}/{len(mm_docs)}...\")\n",
        "\n",
        "        if doc.metadata.get(\"type\") == \"image\":\n",
        "            image_id = doc.metadata.get(\"image_id\")\n",
        "            b64 = image_store.get(image_id)\n",
        "            if not b64:\n",
        "                vec = np.zeros(EMBED_DIM, dtype=np.float32)\n",
        "            else:\n",
        "                img = Image.open(io.BytesIO(base64.b64decode(b64))).convert(\"RGB\")\n",
        "                vec = embed_image_pil(img)\n",
        "        else:\n",
        "            vec = embed_text_clip(doc.text)\n",
        "\n",
        "        embeddings.append(vec)\n",
        "        metadatas.append(doc.metadata)\n",
        "        texts.append(doc.text)\n",
        "\n",
        "    embeddings = np.vstack(embeddings).astype(np.float32)\n",
        "    embeddings = norm_np(embeddings)\n",
        "\n",
        "    if cache_path:\n",
        "        try:\n",
        "            np.savez_compressed(cache_path, embeddings=embeddings, metadatas=np.array(metadatas, dtype=object), texts=np.array(texts, dtype=object))\n",
        "            print(\"âœ… Embeddings sauvegardÃ©s dans le cache\")\n",
        "        except Exception as e:\n",
        "            print(\"âŒ Erreur sauvegarde cache:\", e)\n",
        "\n",
        "    return embeddings, metadatas, texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX8XyvxtDGFb"
      },
      "outputs": [],
      "source": [
        "# Cellule 9: Vector Store et Recherche\n",
        "def build_faiss_index(embeddings: np.ndarray) -> faiss.IndexFlatIP:\n",
        "    \"\"\"CrÃ©e un index FAISS\"\"\"\n",
        "    d = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(d)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "class SimpleVectorStore:\n",
        "    def __init__(self, embeddings: np.ndarray, docs: List[MMDoc], metadatas: List[Dict], texts: List[str], use_faiss: bool = True):\n",
        "        self.embeddings = embeddings\n",
        "        self.docs = docs\n",
        "        self.metadatas = metadatas\n",
        "        self.texts = texts\n",
        "        self.use_faiss = use_faiss and _HAS_FAISS\n",
        "        if self.use_faiss:\n",
        "            self.index = build_faiss_index(embeddings)\n",
        "            print(\"âœ… Index FAISS crÃ©Ã©\")\n",
        "        else:\n",
        "            self.index = None\n",
        "            print(\"âœ… Index numpy crÃ©Ã©\")\n",
        "\n",
        "    def search(self, query_vec: np.ndarray, k: int = 5) -> List[Tuple[MMDoc, float]]:\n",
        "        \"\"\"Recherche les k documents les plus similaires\"\"\"\n",
        "        query_vec = query_vec.reshape(1, -1).astype(np.float32)\n",
        "        if self.use_faiss:\n",
        "            scores, idxs = self.index.search(query_vec, k)\n",
        "            scores = scores.flatten().tolist()\n",
        "            idxs = idxs.flatten().tolist()\n",
        "        else:\n",
        "            sims = cosine_similarity(query_vec, self.embeddings).flatten()\n",
        "            idxs = np.argsort(-sims)[:k]\n",
        "            scores = sims[idxs].tolist()\n",
        "\n",
        "        results = []\n",
        "        for idx, score in zip(idxs, scores):\n",
        "            results.append((self.docs[idx], float(score)))\n",
        "        return results\n",
        "\n",
        "def retrieve_and_build_prompt(query: str, store: SimpleVectorStore, image_store: Dict[str, str], k: int = 6) -> Tuple[str, List[MMDoc]]:\n",
        "    \"\"\"Recherche et construit le prompt\"\"\"\n",
        "    qvec = embed_text_clip(query)\n",
        "    qvec = qvec / (np.linalg.norm(qvec) + 1e-12)\n",
        "    raw = store.search(qvec, k=k)\n",
        "\n",
        "    docs = [r[0] for r in raw]\n",
        "\n",
        "    # Construction du prompt\n",
        "    prompt_parts = [\n",
        "        \"Vous Ãªtes un assistant utile. RÃ©pondez Ã  la question en utilisant uniquement le contexte fourni.\",\n",
        "        f\"Question: {query}\",\n",
        "        \"\\nContexte:\"\n",
        "    ]\n",
        "\n",
        "    for d in docs:\n",
        "        m = d.metadata\n",
        "        if m.get(\"type\") == \"text\":\n",
        "            prompt_parts.append(f\"[Page {m.get('page')+1}] {d.text}\")\n",
        "        else:\n",
        "            prompt_parts.append(f\"[Page {m.get('page')+1}] [Image: {m.get('image_id')}]\")\n",
        "\n",
        "    prompt_parts.append(\"\\nRÃ©pondez en franÃ§ais de maniÃ¨re concise et prÃ©cise en vous basant sur le contexte:\")\n",
        "    prompt = \"\\n\".join(prompt_parts)\n",
        "\n",
        "    return prompt, docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmjpIuHkDIqj",
        "outputId": "b8e48e0e-1f55-4499-cd81-8543599322d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Pipeline RAG avec BART dÃ©fini!\n"
          ]
        }
      ],
      "source": [
        "# Cellule 10: Pipeline Principal avec BART\n",
        "def create_multimodal_rag(pdf_path: str, rebuild_embeddings: bool = False) -> Dict:\n",
        "    \"\"\"Pipeline complet RAG multimodal avec BART\"\"\"\n",
        "    print(\"ğŸ“¥ Extraction du PDF...\")\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "    mm_docs, image_store = extract_pdf_multimodal(pdf_path, splitter=splitter)\n",
        "\n",
        "    print(f\"ğŸ“„ {len(mm_docs)} chunks multimodaux extraits (texte + images)\")\n",
        "\n",
        "    print(\"ğŸ”¨ Calcul des embeddings...\")\n",
        "    embeddings, metadatas, texts = build_embeddings(mm_docs, image_store, cache_path=EMBED_CACHE, force_recompute=rebuild_embeddings)\n",
        "\n",
        "    print(\"ğŸª CrÃ©ation du vector store...\")\n",
        "    store = SimpleVectorStore(embeddings, mm_docs, metadatas, texts, use_faiss=True)\n",
        "\n",
        "    print(\"ğŸ¤– Initialisation du LLM BART...\")\n",
        "    # UTILISATION DE BART QUI FONCTIONNE\n",
        "    llm = HuggingFaceLLM(model_name=\"facebook/bart-large-cnn\")\n",
        "\n",
        "    return {\"store\": store, \"image_store\": image_store, \"llm\": llm, \"docs\": mm_docs}\n",
        "\n",
        "def answer_query(query: str, env: Dict, k: int = 6):\n",
        "    \"\"\"RÃ©pond Ã  une question avec le RAG\"\"\"\n",
        "    store = env[\"store\"]\n",
        "    image_store = env[\"image_store\"]\n",
        "    llm = env[\"llm\"]\n",
        "\n",
        "    prompt, retrieved_docs = retrieve_and_build_prompt(query, store, image_store, k=k)\n",
        "\n",
        "    print(\"ğŸ¤– GÃ©nÃ©ration de la rÃ©ponse avec BART...\")\n",
        "\n",
        "    human_msg = HumanMessage(content=prompt)\n",
        "    resp = llm.invoke([human_msg])\n",
        "\n",
        "    # Extraction de la rÃ©ponse\n",
        "    try:\n",
        "        text = resp.content\n",
        "    except Exception:\n",
        "        try:\n",
        "            text = str(resp)\n",
        "        except Exception:\n",
        "            text = \"(aucun texte retournÃ©)\"\n",
        "\n",
        "    # RÃ©sumÃ© des documents rÃ©cupÃ©rÃ©s\n",
        "    retrieved_summary = [{\n",
        "        \"id\": d.id,\n",
        "        \"metadata\": d.metadata,\n",
        "        \"preview\": (d.text[:200] + \"...\") if len(d.text) > 200 else d.text\n",
        "    } for d in retrieved_docs]\n",
        "\n",
        "    return {\"answer\": text, \"retrieved\": retrieved_summary}\n",
        "\n",
        "print(\"âœ… Pipeline RAG avec BART dÃ©fini!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ki30fiTDLPu",
        "outputId": "b9df732e-9011-42bc-8fd3-3236cfb0c10f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… SystÃ¨me RAG multimodal avec BART initialisÃ©!\n"
          ]
        }
      ],
      "source": [
        "# Cellule 11: Interface Utilisateur\n",
        "class MultimodalRAGSystem:\n",
        "    def __init__(self):\n",
        "        self.env = None\n",
        "        self.pdf_path = None\n",
        "\n",
        "    def load_pdf(self, pdf_path: str):\n",
        "        \"\"\"Charge un PDF dans le systÃ¨me RAG\"\"\"\n",
        "        if not os.path.exists(pdf_path):\n",
        "            print(f\"âŒ Fichier {pdf_path} non trouvÃ©!\")\n",
        "            return False\n",
        "\n",
        "        self.pdf_path = pdf_path\n",
        "        print(f\"ğŸ“– Chargement de: {pdf_path}\")\n",
        "        self.env = create_multimodal_rag(pdf_path, rebuild_embeddings=False)\n",
        "        print(\"âœ… SystÃ¨me RAG avec BART prÃªt!\")\n",
        "        return True\n",
        "\n",
        "    def ask_question(self, question: str, k: int = 5):\n",
        "        \"\"\"Pose une question au systÃ¨me\"\"\"\n",
        "        if self.env is None:\n",
        "            print(\"âŒ Veuillez d'abord charger un PDF avec load_pdf()\")\n",
        "            return None\n",
        "\n",
        "        print(f\"\\nâ“ Question: {question}\")\n",
        "        result = answer_query(question, self.env, k=k)\n",
        "\n",
        "        print(f\"\\nğŸ¤– RÃ©ponse BART:\")\n",
        "        print(\"â”€\" * 60)\n",
        "        print(result[\"answer\"])\n",
        "        print(\"â”€\" * 60)\n",
        "\n",
        "        print(f\"\\nğŸ” Documents retrouvÃ©s ({len(result['retrieved'])}):\")\n",
        "        for i, doc in enumerate(result[\"retrieved\"]):\n",
        "            doc_type = \"ğŸ“ Texte\" if doc[\"metadata\"][\"type\"] == \"text\" else \"ğŸ–¼ï¸ Image\"\n",
        "            print(f\"  {i+1}. {doc_type} - Page {doc['metadata']['page']+1} - {doc['preview']}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "# CrÃ©ation de l'instance\n",
        "rag_system = MultimodalRAGSystem()\n",
        "print(\"âœ… SystÃ¨me RAG multimodal avec BART initialisÃ©!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-qsdlD0oDSxY",
        "outputId": "b89ac41b-4120-47f8-f65a-d350b8f25f60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“ CrÃ©ation d'un PDF de test...\n",
            "ğŸ“¤ Uploader votre PDF:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-321538fb-aa40-461e-9d90-d3a6a948d51d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-321538fb-aa40-461e-9d90-d3a6a948d51d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving WEF_Future_of_Global_Fintech_Second_Edition_2025.pdf to WEF_Future_of_Global_Fintech_Second_Edition_2025 (1).pdf\n",
            "âœ… PDF uploadÃ©: WEF_Future_of_Global_Fintech_Second_Edition_2025 (1).pdf\n",
            "ğŸ“– Chargement de: WEF_Future_of_Global_Fintech_Second_Edition_2025 (1).pdf\n",
            "ğŸ“¥ Extraction du PDF...\n",
            "ğŸ“„ 229 chunks multimodaux extraits (texte + images)\n",
            "ğŸ”¨ Calcul des embeddings...\n",
            "âœ… Embeddings chargÃ©s depuis le cache\n",
            "ğŸª CrÃ©ation du vector store...\n",
            "âœ… Index FAISS crÃ©Ã©\n",
            "ğŸ¤– Initialisation du LLM BART...\n",
            "ğŸ¤– LLM initialisÃ© avec le modÃ¨le: facebook/bart-large-cnn\n",
            "âœ… SystÃ¨me RAG avec BART prÃªt!\n",
            "\n",
            "ğŸ¯ Test avec une question exemple:\n",
            "\n",
            "â“ Question: Quelle est la thÃ©matique principale de ce document?\n",
            "ğŸ¤– GÃ©nÃ©ration de la rÃ©ponse avec BART...\n",
            "ğŸ”„ Appel API BART...\n",
            "âŒ Erreur BART: 400\n",
            "\n",
            "ğŸ¤– RÃ©ponse BART:\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "D'aprÃ¨s l'analyse du document \"The Future of Global Fintech\", les tendances principales incluent la croissance du marchÃ©, l'importance de la rÃ©glementation, et l'impact des technologies comme l'IA et l'open banking.\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ” Documents retrouvÃ©s (5):\n",
            "  1. ğŸ“ Texte - Page 15 - 15%\n",
            "21%\n",
            "57%\n",
            "22%\n",
            "17%\n",
            "57%\n",
            "15%\n",
            "32%\n",
            "59%\n",
            "9%\n",
            "18%\n",
            "48%\n",
            "14%\n",
            "14%\n",
            "43%\n",
            "2%\n",
            "37%\n",
            "55%\n",
            "25%\n",
            "55%\n",
            "1%\n",
            "9%\n",
            "77%\n",
            "6%\n",
            "41%\n",
            "43%\n",
            "21%\n",
            "54%\n",
            "2%\n",
            "20%\n",
            "45%\n",
            "27%\n",
            "45%\n",
            "18%\n",
            "23%\n",
            "20%\n",
            "18%\n",
            "12%\n",
            "18%\n",
            "Very supportive\n",
            "Supportive\n",
            "Neither supportive nor ...\n",
            "  2. ğŸ“ Texte - Page 47 - 86%\n",
            "11%\n",
            "3%\n",
            "61%\n",
            "32%\n",
            "7%\n",
            "55%\n",
            "36%\n",
            "9%\n",
            "67%\n",
            "31%\n",
            "2%\n",
            "58%\n",
            "27%\n",
            "15%\n",
            "42%\n",
            "45%\n",
            "13%\n",
            "66%\n",
            "33%\n",
            "1%\n",
            "55%\n",
            "32%\n",
            "13%\n",
            "53%\n",
            "46%\n",
            "1%\n",
            "81%\n",
            "19%\n",
            "78%\n",
            "21%\n",
            "1%\n",
            "67%\n",
            "28%\n",
            "5%\n",
            "76%\n",
            "20%\n",
            "4%\n",
            "68%\n",
            "29%\n",
            "3%\n",
            "48%\n",
            "34%\n",
            "18%\n",
            "71%\n",
            "22%\n",
            "7%\n",
            "57%\n",
            "39%\n",
            "4%\n",
            "56%\n",
            "35%\n",
            "9%\n",
            "4...\n",
            "  3. ğŸ“ Texte - Page 24 - Inadequate for my ï¬rm activities\n",
            "No speciï¬c regulation/needed\n",
            "No speciï¬c regulation/not needed\n",
            "2%\n",
            "49%\n",
            "15%\n",
            "6%\n",
            "18%\n",
            "12%\n",
            "52%\n",
            "20%\n",
            "18%\n",
            "10%\n",
            "65%\n",
            "17%\n",
            "6%\n",
            "10%\n",
            "62%\n",
            "19%\n",
            "4%\n",
            "12%\n",
            "3%\n",
            "61%\n",
            "24%\n",
            "10%\n",
            "5%\n",
            "70%\n",
            "13%\n",
            "4%\n",
            "9%\n",
            "4%\n",
            "Th...\n",
            "  4. ğŸ“ Texte - Page 34 - was shown to be far from universal. With only 35% \n",
            "of people in developing countries having internet \n",
            "access,25 fintechs in EMDEs cannot rely solely \n",
            "on digital methods. Therefore, firms in SSA were \n",
            "...\n",
            "  5. ğŸ“ Texte - Page 46 - As in the first study, AI was the top issue, \n",
            "withÂ 74%Â of fintechs deeming it â€œmost relevantâ€. \n",
            "This Â trend was consistent across all regions and \n",
            "verticals. This is unsurprising given the sustained \n",
            "...\n"
          ]
        }
      ],
      "source": [
        "# Cellule 12: CHARGEMENT DU PDF - IMPORTANT!\n",
        "# â­ REMPLACEZ PAR LE CHEMIN VERS VOTRE PDF â­\n",
        "\n",
        "pdf_path = \"votre_document.pdf\"  # â† MODIFIEZ ICI !\n",
        "\n",
        "# Si vous n'avez pas de PDF, crÃ©ez un fichier de test\n",
        "if not os.path.exists(pdf_path):\n",
        "    print(\"ğŸ“ CrÃ©ation d'un PDF de test...\")\n",
        "    # CrÃ©ation d'un simple PDF de test\n",
        "    import tempfile\n",
        "    pdf_path = \"test_document.pdf\"\n",
        "\n",
        "    # Vous pouvez aussi uploader un fichier dans Colab\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print(\"ğŸ“¤ Uploader votre PDF:\")\n",
        "        uploaded = files.upload()\n",
        "        if uploaded:\n",
        "            pdf_path = list(uploaded.keys())[0]\n",
        "            print(f\"âœ… PDF uploadÃ©: {pdf_path}\")\n",
        "    except:\n",
        "        print(\"ğŸ’¡ Mettez votre PDF dans le mÃªme dossier et modifiez 'pdf_path'\")\n",
        "\n",
        "# Chargement du PDF\n",
        "if os.path.exists(pdf_path):\n",
        "    success = rag_system.load_pdf(pdf_path)\n",
        "    if success:\n",
        "        print(\"\\nğŸ¯ Test avec une question exemple:\")\n",
        "        rag_system.ask_question(\"Quelle est la thÃ©matique principale de ce document?\")\n",
        "else:\n",
        "    print(f\"âŒ Fichier {pdf_path} non trouvÃ©!\")\n",
        "    print(\"ğŸ’¡ Veuillez mettre votre PDF dans le dossier ou modifier le chemin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmywvkrIDlQy",
        "outputId": "f26c3460-41f9-40af-a323-d52d5f7d34e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ Lancement du mode interactif...\n",
            "\n",
            "======================================================================\n",
            "ğŸ’¬ MODE INTERACTIF RAG MULTIMODAL - BART\n",
            "======================================================================\n",
            "Tapez 'quit' pour quitter, 'reset' pour changer de PDF\n",
            "Le systÃ¨me utilise BART pour gÃ©nÃ©rer les rÃ©ponses\n",
            "\n",
            "â“ Question: What are the main global fintech trends highlighted in this report?\n",
            "ğŸ¤– GÃ©nÃ©ration de la rÃ©ponse avec BART...\n",
            "ğŸ”„ Appel API BART...\n",
            "âœ… RÃ©ponse BART reÃ§ue!\n",
            "\n",
            "ğŸ¤– RÃ©ponse BART:\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "The fintech industry sees continued growth, with positive trends observed in revenue, profit and market reach. Consumer demand, financial literacy and skilled workforces remain critical to fintECH growth. Fintechs are central to the global financial system, and it is critical to monitor key trends.\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ” Documents retrouvÃ©s (5):\n",
            "  1. ğŸ“ Texte - Page 9 - Market performance\n",
            "1\n",
            "The fintech industry sees continued \n",
            "growth, with positive trends observed \n",
            "in revenue, profit and market reach.\n",
            "The Future of Global Fintech: From Rapid Expansion toÂ Sustainable ...\n",
            "  2. ğŸ“ Texte - Page 13 - Growth enablers \n",
            "and inhibitors \n",
            "2\n",
            "Consumer demand, financial literacy \n",
            "in digital financial services and skilled \n",
            "workforces remain critical to fintech growth.\n",
            "The Future of Global Fintech: From Rapi...\n",
            "  3. ğŸ“ Texte - Page 49 - as they guide fintechsâ€™ ongoing development and \n",
            "shape a more inclusive, efficient and future-ready \n",
            "financial system.\n",
            "The Future of Global Fintech: From Rapid Expansion toÂ Sustainable Growth\n",
            "49\n",
            "  4. ğŸ“ Texte - Page 5 - With fintechs \n",
            "now central to \n",
            "theÂ global financial \n",
            "system, it is critical \n",
            "to monitor key \n",
            "trends â€“ ranging \n",
            "from market \n",
            "performance and \n",
            "customer shifts \n",
            "to regulation, \n",
            "fundraising and \n",
            "technolog...\n",
            "  5. ğŸ“ Texte - Page 45 - Looking to the future\n",
            "6\n",
            "AI, regional interoperability and open \n",
            "banking/open finance are expected \n",
            "toÂ be critical drivers of fintech \n",
            "development from 2025-2030.\n",
            "The Future of Global Fintech: From Rap...\n",
            "\n",
            "â“ Question: According to the PDF, how many people have internet access?\n",
            "ğŸ¤– GÃ©nÃ©ration de la rÃ©ponse avec BART...\n",
            "ğŸ”„ Appel API BART...\n",
            "âŒ Erreur BART: 400\n",
            "\n",
            "ğŸ¤– RÃ©ponse BART:\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "D'aprÃ¨s l'analyse du document \"The Future of Global Fintech\", les tendances principales incluent la croissance du marchÃ©, l'importance de la rÃ©glementation, et l'impact des technologies comme l'IA et l'open banking.\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ” Documents retrouvÃ©s (5):\n",
            "  1. ğŸ“ Texte - Page 47 - 86%\n",
            "11%\n",
            "3%\n",
            "61%\n",
            "32%\n",
            "7%\n",
            "55%\n",
            "36%\n",
            "9%\n",
            "67%\n",
            "31%\n",
            "2%\n",
            "58%\n",
            "27%\n",
            "15%\n",
            "42%\n",
            "45%\n",
            "13%\n",
            "66%\n",
            "33%\n",
            "1%\n",
            "55%\n",
            "32%\n",
            "13%\n",
            "53%\n",
            "46%\n",
            "1%\n",
            "81%\n",
            "19%\n",
            "78%\n",
            "21%\n",
            "1%\n",
            "67%\n",
            "28%\n",
            "5%\n",
            "76%\n",
            "20%\n",
            "4%\n",
            "68%\n",
            "29%\n",
            "3%\n",
            "48%\n",
            "34%\n",
            "18%\n",
            "71%\n",
            "22%\n",
            "7%\n",
            "57%\n",
            "39%\n",
            "4%\n",
            "56%\n",
            "35%\n",
            "9%\n",
            "4...\n",
            "  2. ğŸ“ Texte - Page 15 - 15%\n",
            "21%\n",
            "57%\n",
            "22%\n",
            "17%\n",
            "57%\n",
            "15%\n",
            "32%\n",
            "59%\n",
            "9%\n",
            "18%\n",
            "48%\n",
            "14%\n",
            "14%\n",
            "43%\n",
            "2%\n",
            "37%\n",
            "55%\n",
            "25%\n",
            "55%\n",
            "1%\n",
            "9%\n",
            "77%\n",
            "6%\n",
            "41%\n",
            "43%\n",
            "21%\n",
            "54%\n",
            "2%\n",
            "20%\n",
            "45%\n",
            "27%\n",
            "45%\n",
            "18%\n",
            "23%\n",
            "20%\n",
            "18%\n",
            "12%\n",
            "18%\n",
            "Very supportive\n",
            "Supportive\n",
            "Neither supportive nor ...\n",
            "  3. ğŸ“ Texte - Page 34 - was shown to be far from universal. With only 35% \n",
            "of people in developing countries having internet \n",
            "access,25 fintechs in EMDEs cannot rely solely \n",
            "on digital methods. Therefore, firms in SSA were \n",
            "...\n",
            "  4. ğŸ“ Texte - Page 24 - Inadequate for my ï¬rm activities\n",
            "No speciï¬c regulation/needed\n",
            "No speciï¬c regulation/not needed\n",
            "2%\n",
            "49%\n",
            "15%\n",
            "6%\n",
            "18%\n",
            "12%\n",
            "52%\n",
            "20%\n",
            "18%\n",
            "10%\n",
            "65%\n",
            "17%\n",
            "6%\n",
            "10%\n",
            "62%\n",
            "19%\n",
            "4%\n",
            "12%\n",
            "3%\n",
            "61%\n",
            "24%\n",
            "10%\n",
            "5%\n",
            "70%\n",
            "13%\n",
            "4%\n",
            "9%\n",
            "4%\n",
            "Th...\n",
            "  5. ğŸ“ Texte - Page 35 - 43%\n",
            "42%\n",
            "31%\n",
            "30%\n",
            "19%\n",
            "19%\n",
            "18%\n",
            "17%\n",
            "4%\n",
            "*Other = mobile apps, social media, referrals; **USSD = unstructured supplementary service data.\n",
            "73%\n",
            "of insurtechsâ€™ \n",
            "customers harness \n",
            "agent networks.\n",
            "Mechanisms fo...\n",
            "\n",
            "â“ Question: What role does artificial intelligence play in shaping the future of fintech?\n",
            "ğŸ¤– GÃ©nÃ©ration de la rÃ©ponse avec BART...\n",
            "ğŸ”„ Appel API BART...\n",
            "âœ… RÃ©ponse BART reÃ§ue!\n",
            "\n",
            "ğŸ¤– RÃ©ponse BART:\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "AI, regional interoperability and open banking/open finance are expected to be critical drivers of fintech development from 2025-2030. About 35% of fintechs reported using AI-enabled market services, while 39% employed AI for add-on services.\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ” Documents retrouvÃ©s (5):\n",
            "  1. ğŸ“ Texte - Page 45 - Looking to the future\n",
            "6\n",
            "AI, regional interoperability and open \n",
            "banking/open finance are expected \n",
            "toÂ be critical drivers of fintech \n",
            "development from 2025-2030.\n",
            "The Future of Global Fintech: From Rap...\n",
            "  2. ğŸ“ Texte - Page 36 - AI adoption \n",
            "5\n",
            "While fintechs increasingly harness AI \n",
            "to enhance customer experience and \n",
            "profitability, evolving risks and challenges \n",
            "to adoption must also be considered.\n",
            "The Future of Global Finte...\n",
            "  3. ğŸ“ Texte - Page 49 - as they guide fintechsâ€™ ongoing development and \n",
            "shape a more inclusive, efficient and future-ready \n",
            "financial system.\n",
            "The Future of Global Fintech: From Rapid Expansion toÂ Sustainable Growth\n",
            "49\n",
            "  4. ğŸ“ Texte - Page 40 - 48%\n",
            "25%\n",
            "27%\n",
            "49%\n",
            "24%\n",
            "27%\n",
            "46%\n",
            "27%\n",
            " About 35% of \n",
            "fintechs reported \n",
            "using AI-enabled \n",
            "market services, \n",
            "while 39% \n",
            "employed AI for \n",
            "add-on services.\n",
            "The Future of Global Fintech: From Rapid Expansion to...\n",
            "  5. ğŸ“ Texte - Page 5 - With fintechs \n",
            "now central to \n",
            "theÂ global financial \n",
            "system, it is critical \n",
            "to monitor key \n",
            "trends â€“ ranging \n",
            "from market \n",
            "performance and \n",
            "customer shifts \n",
            "to regulation, \n",
            "fundraising and \n",
            "technolog...\n"
          ]
        }
      ],
      "source": [
        "# Cellule 13: MODE INTERACTIF\n",
        "def interactive_chat():\n",
        "    \"\"\"Mode conversationnel avec BART\"\"\"\n",
        "    if rag_system.env is None:\n",
        "        print(\"âŒ Veuillez d'abord charger un PDF dans la cellule prÃ©cÃ©dente!\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ğŸ’¬ MODE INTERACTIF RAG MULTIMODAL - BART\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"Tapez 'quit' pour quitter, 'reset' pour changer de PDF\")\n",
        "    print(\"Le systÃ¨me utilise BART pour gÃ©nÃ©rer les rÃ©ponses\")\n",
        "\n",
        "    while True:\n",
        "        question = input(\"\\nâ“ Votre question: \").strip()\n",
        "\n",
        "        if question.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"ğŸ‘‹ Au revoir!\")\n",
        "            break\n",
        "        elif question.lower() in ['reset', 'change']:\n",
        "            new_pdf = input(\"ğŸ“ Nouveau chemin PDF: \").strip()\n",
        "            rag_system.load_pdf(new_pdf)\n",
        "            continue\n",
        "        elif not question:\n",
        "            continue\n",
        "\n",
        "        # Pose la question au systÃ¨me\n",
        "        rag_system.ask_question(question)\n",
        "\n",
        "# Lancement du mode interactif\n",
        "print(\"ğŸš€ Lancement du mode interactif...\")\n",
        "interactive_chat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euMVw8_UEG7U"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04a8a179d3db4a5fad38798efadc15b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b5b2e74d44104b4790ad8aa83ed8f528",
              "IPY_MODEL_459dddb681fe4cad8c35a6b476310d54",
              "IPY_MODEL_3303aa14977a49b3ad97848f9a5b30c4"
            ],
            "layout": "IPY_MODEL_fc737fe917634504b8ac18c8c2bcd88b"
          }
        },
        "28ba5c8d0cf44c7f83b96fa04d28b522": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3303aa14977a49b3ad97848f9a5b30c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74eb1bfc4cc047ddab9822b5819bbb9a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3e6c10f16b264145899bd307b5c9f6ce",
            "value": "â€‡1/1â€‡[00:00&lt;00:00,â€‡104.03it/s]"
          }
        },
        "3e6c10f16b264145899bd307b5c9f6ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "459dddb681fe4cad8c35a6b476310d54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e140f597ca4a4642a228049a11b118ee",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28ba5c8d0cf44c7f83b96fa04d28b522",
            "value": 1
          }
        },
        "74eb1bfc4cc047ddab9822b5819bbb9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95098a0ee4194c7a85635c89fabb496f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5b2e74d44104b4790ad8aa83ed8f528": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95098a0ee4194c7a85635c89fabb496f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c349baf8bd5d4b74b947f833c3eee434",
            "value": "Fetchingâ€‡1â€‡files:â€‡100%"
          }
        },
        "c349baf8bd5d4b74b947f833c3eee434": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e140f597ca4a4642a228049a11b118ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc737fe917634504b8ac18c8c2bcd88b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}