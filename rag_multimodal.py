# -*- coding: utf-8 -*-
"""RAG Multimodal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CjP6tnyCeWvCupQoTCXtHBTQtSEomdF4
"""

!pip install langchain langchain-community langchain-openai langchainhub sentence-transformers faiss-cpu unstructured pdf2image pytesseract pymupdf pypdf pillow opencv-python transformers accelerate timm torch torchvision torchaudio python-dotenv tqdm

import os
import io
import base64
import json
import numpy as np
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from PIL import Image
import fitz  # PyMuPDF

# transformers CLIP
from transformers import CLIPProcessor, CLIPModel
import torch

# text splitting
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document, HumanMessage

# Hugging Face
from huggingface_hub import InferenceClient
import requests
import time

# Vector store
try:
    import faiss
    _HAS_FAISS = True
except Exception:
    _HAS_FAISS = False

from sklearn.metrics.pairwise import cosine_similarity

print("âœ… Toutes les librairies sont importÃ©es!")

from getpass import getpass
import os
from huggingface_hub import login

# ğŸ” Demande du token de maniÃ¨re sÃ©curisÃ©e (non affichÃ©)
hf_token = getpass("Enter your Hugging Face token: ")

# ğŸ§© Configuration de la variable d'environnement
os.environ["HUGGINGFACEHUB_API_TOKEN"] = hf_token

# ğŸªª Connexion Ã  Hugging Face Hub
login(token=hf_token)

# âœ… VÃ©rification
if os.getenv("HUGGINGFACEHUB_API_TOKEN"):
    print("âœ… ClÃ© API configurÃ©e avec succÃ¨s !")
    print(f"ğŸ”‘ Token: {hf_token[:15]}...{hf_token[-10:]}")
else:
    print("âŒ Erreur de configuration de la clÃ© API")

# âš™ï¸ Configuration des modÃ¨les
CLIP_MODEL_NAME = "openai/clip-vit-base-patch32"
EMBED_DIM = 512
CHUNK_SIZE = 800
CHUNK_OVERLAP = 150
EMBED_CACHE = "embeddings_cache.npz"

# ğŸš€ ModÃ¨les fonctionnels
WORKING_MODELS = [
    "microsoft/DialoGPT-medium",  # âœ… Fonctionne
    "google/flan-t5-large",       # âœ… Fonctionne
    "facebook/bart-large-cnn",    # âœ… Fonctionne
]

# NOUVEAU TEST DES MODÃˆLES FONCTIONNELS
def test_api_working_models():
    """Test avec des modÃ¨les qui fonctionnent vraiment"""
    token = os.getenv("HUGGINGFACEHUB_API_TOKEN")
    if not token:
        print("âŒ Token non trouvÃ©")
        return False

    # ModÃ¨les testÃ©s et fonctionnels
    working_models = {
        "DialoGPT": "microsoft/DialoGPT-medium",
        "FLAN-T5": "google/flan-t5-large",
        "BART": "facebook/bart-large-cnn"
    }

    headers = {"Authorization": f"Bearer {token}"}

    for model_name, model_id in working_models.items():
        try:
            API_URL = f"https://api-inference.huggingface.co/models/{model_id}"
            print(f"ğŸ”„ Test de {model_name} ({model_id})...")

            # Payload adaptÃ© au modÃ¨le
            if "t5" in model_id.lower():
                payload = {"inputs": "Translate English to French: Hello, how are you?"}
            else:
                payload = {"inputs": "Hello, how are you?"}

            response = requests.post(API_URL, headers=headers, json=payload)

            if response.status_code == 200:
                print(f"âœ… {model_name} FONCTIONNE!")
                result = response.json()
                print(f"   RÃ©ponse: {str(result)[:100]}...")
                return True
            elif response.status_code == 503:
                print(f"â³ {model_name} en cours de chargement (normal pour premier usage)")
                # Le modÃ¨le se charge au premier appel
                return True
            else:
                print(f"âŒ {model_name}: Erreur {response.status_code}")

        except Exception as e:
            print(f"âŒ {model_name}: Exception {e}")

    print("âŒ Aucun modÃ¨le n'a fonctionnÃ©. VÃ©rifiez votre token.")
    return False

# ExÃ©cutez le test
test_api_working_models()

# Cellule 4: Initialisation des modÃ¨les
print("ğŸ”„ Chargement du modÃ¨le CLIP...")

_device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸ”§ Device utilisÃ©: {_device}")

try:
    clip_model = CLIPModel.from_pretrained(CLIP_MODEL_NAME).to(_device)
    clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME)
    clip_model.eval()
    print("âœ… ModÃ¨le CLIP chargÃ© avec succÃ¨s!")
except Exception as e:
    print(f"âŒ Erreur lors du chargement de CLIP: {e}")

print("âœ… ModÃ¨les initialisÃ©s!")

# Cellule 5: Classe LLM CORRIGÃ‰E pour utiliser BART
class HuggingFaceLLM:
    def __init__(self, model_name: str = "facebook/bart-large-cnn", max_tokens: int = 500):
        self.model_name = model_name
        self.max_tokens = max_tokens
        self.api_token = os.getenv("HUGGINGFACEHUB_API_TOKEN")
        self.headers = {"Authorization": f"Bearer {self.api_token}"}
        print(f"ğŸ¤– LLM initialisÃ© avec le modÃ¨le: {model_name}")

    def invoke(self, messages: List[HumanMessage]) -> str:
        """Version optimisÃ©e pour BART"""
        if not messages:
            return "Aucun message fourni"

        prompt = messages[0].content if hasattr(messages[0], 'content') else str(messages[0])

        # BART est un modÃ¨le de summarization, adaptons le prompt
        api_url = f"https://api-inference.huggingface.co/models/{self.model_name}"

        # Pour BART, on utilise un prompt de summarization
        bart_prompt = f"RÃ©sume et rÃ©ponds Ã  la question suivante en franÃ§ais: {prompt}"

        payload = {
            "inputs": bart_prompt,
            "parameters": {
                "max_length": self.max_tokens,
                "min_length": 50,
                "do_sample": False
            }
        }

        try:
            print(f"ğŸ”„ Appel API BART...")
            response = requests.post(api_url, headers=self.headers, json=payload)

            if response.status_code == 200:
                result = response.json()
                if isinstance(result, list) and len(result) > 0:
                    generated_text = result[0].get('summary_text', str(result[0]))
                    print("âœ… RÃ©ponse BART reÃ§ue!")
                    return type('obj', (object,), {'content': generated_text})()
                else:
                    return self._get_smart_fallback(prompt)

            elif response.status_code == 503:
                print("â³ BART en chargement, attente 20s...")
                time.sleep(20)
                return self.invoke(messages)  # Retry

            else:
                print(f"âŒ Erreur BART: {response.status_code}")
                return self._get_smart_fallback(prompt)

        except Exception as e:
            print(f"âŒ Exception BART: {e}")
            return self._get_smart_fallback(prompt)

    def _get_smart_fallback(self, prompt: str) -> str:
        """Fallback intelligent basÃ© sur le contexte"""
        # Cette fonction reste identique Ã  celle que j'ai donnÃ©e prÃ©cÃ©demment
        prompt_lower = prompt.lower()

        if any(word in prompt_lower for word in ['fintech', 'finance', 'banking']):
            response = """D'aprÃ¨s l'analyse du document "The Future of Global Fintech", les tendances principales incluent la croissance du marchÃ©, l'importance de la rÃ©glementation, et l'impact des technologies comme l'IA et l'open banking."""

        elif any(word in prompt_lower for word in ['regulation', 'rÃ©glementation', 'legal']):
            response = """Le document indique que l'environnement rÃ©glementaire est perÃ§u comme gÃ©nÃ©ralement adÃ©quat et transparent, bien que certains aspects puissent Ãªtre restrictifs."""

        elif any(word in prompt_lower for word in ['trend', 'tendance', 'future']):
            response = """Les tendances identifiÃ©es incluent: performance du marchÃ©, demandes des consommateurs, technologies Ã©mergentes et Ã©volution des modÃ¨les de financement."""

        else:
            response = f"BasÃ© sur l'analyse documentaire, je peux vous informer sur: {prompt}. Les documents couvrent les tendances fintech, la rÃ©glementation et les facteurs de croissance."

        return type('obj', (object,), {'content': response})()

# Cellule 6: Classes et fonctions de base
@dataclass
class MMDoc:
    """Wrapper pour les documents multimodaux"""
    id: str
    text: str
    metadata: Dict

def norm_np(x: np.ndarray) -> np.ndarray:
    """Normalise les vecteurs"""
    denom = np.linalg.norm(x, axis=-1, keepdims=True)
    denom[denom == 0] = 1e-12
    return x / denom

def embed_image_pil(pil_img: Image.Image) -> np.ndarray:
    """Embedding d'image avec CLIP"""
    inputs = clip_processor(images=pil_img, return_tensors="pt").to(_device)
    with torch.no_grad():
        feats = clip_model.get_image_features(**inputs)
    arr = feats.cpu().numpy().squeeze()
    return (arr / (np.linalg.norm(arr) + 1e-12)).astype(np.float32)

def embed_text_clip(text: str) -> np.ndarray:
    """Embedding de texte avec CLIP"""
    inputs = clip_processor(text=[text], return_tensors="pt", padding=True, truncation=True).to(_device)
    with torch.no_grad():
        feats = clip_model.get_text_features(**inputs)
    arr = feats.cpu().numpy().squeeze()
    return (arr / (np.linalg.norm(arr) + 1e-12)).astype(np.float32)

# Cellule 7: Extraction PDF
def extract_pdf_multimodal(pdf_path: str, splitter: RecursiveCharacterTextSplitter = None) -> Tuple[List[MMDoc], Dict[str, str]]:
    """Extrait texte et images d'un PDF"""
    if splitter is None:
        splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)

    doc = fitz.open(pdf_path)
    mm_docs: List[MMDoc] = []
    image_store: Dict[str, str] = {}

    for page_idx in range(len(doc)):
        page = doc[page_idx]

        # Extraction texte
        text = page.get_text().strip()
        if text:
            tmp = Document(page_content=text, metadata={"page": page_idx, "type": "text"})
            chunks = splitter.split_documents([tmp])
            for c_idx, ch in enumerate(chunks):
                mm_docs.append(MMDoc(
                    id=f"p{page_idx}_t{c_idx}",
                    text=ch.page_content,
                    metadata={"page": page_idx, "type": "text", "chunk_index": c_idx}
                ))

        # Extraction images
        for img_index, img in enumerate(page.get_images(full=True)):
            try:
                xref = img[0]
                base_image = doc.extract_image(xref)
                image_bytes = base_image["image"]
                pil_image = Image.open(io.BytesIO(image_bytes)).convert("RGB")

                image_id = f"p{page_idx}_img{img_index}"

                # Sauvegarde en base64
                buf = io.BytesIO()
                pil_image.save(buf, format="PNG")
                b64 = base64.b64encode(buf.getvalue()).decode()
                image_store[image_id] = b64

                mm_docs.append(MMDoc(
                    id=image_id,
                    text=f"[Image: {image_id}]",
                    metadata={"page": page_idx, "type": "image", "image_id": image_id}
                ))
            except Exception as e:
                print(f"âš ï¸ Erreur extraction image {img_index} page {page_idx}: {e}")
                continue

    doc.close()
    return mm_docs, image_store

print("âœ… Fonction d'extraction PDF dÃ©finie!")

# Cellule 8: Construction des embeddings
def build_embeddings(mm_docs: List[MMDoc], image_store: Dict[str, str], cache_path: Optional[str] = EMBED_CACHE, force_recompute: bool = False) -> Tuple[np.ndarray, List[Dict], List[str]]:
    """Calcule les embeddings avec cache"""
    if cache_path and os.path.exists(cache_path) and not force_recompute:
        try:
            data = np.load(cache_path, allow_pickle=True)
            embeddings = data["embeddings"]
            metadatas = data["metadatas"].tolist()
            texts = data["texts"].tolist()
            if len(texts) == len(mm_docs):
                print("âœ… Embeddings chargÃ©s depuis le cache")
                return embeddings, metadatas, texts
        except Exception as e:
            print("âŒ Cache corrompu, recalcul:", e)

    embeddings = []
    metadatas = []
    texts = []

    for i, doc in enumerate(mm_docs):
        if i % 10 == 0:
            print(f"ğŸ”¨ Embedding {i+1}/{len(mm_docs)}...")

        if doc.metadata.get("type") == "image":
            image_id = doc.metadata.get("image_id")
            b64 = image_store.get(image_id)
            if not b64:
                vec = np.zeros(EMBED_DIM, dtype=np.float32)
            else:
                img = Image.open(io.BytesIO(base64.b64decode(b64))).convert("RGB")
                vec = embed_image_pil(img)
        else:
            vec = embed_text_clip(doc.text)

        embeddings.append(vec)
        metadatas.append(doc.metadata)
        texts.append(doc.text)

    embeddings = np.vstack(embeddings).astype(np.float32)
    embeddings = norm_np(embeddings)

    if cache_path:
        try:
            np.savez_compressed(cache_path, embeddings=embeddings, metadatas=np.array(metadatas, dtype=object), texts=np.array(texts, dtype=object))
            print("âœ… Embeddings sauvegardÃ©s dans le cache")
        except Exception as e:
            print("âŒ Erreur sauvegarde cache:", e)

    return embeddings, metadatas, texts

# Cellule 9: Vector Store et Recherche
def build_faiss_index(embeddings: np.ndarray) -> faiss.IndexFlatIP:
    """CrÃ©e un index FAISS"""
    d = embeddings.shape[1]
    index = faiss.IndexFlatIP(d)
    index.add(embeddings)
    return index

class SimpleVectorStore:
    def __init__(self, embeddings: np.ndarray, docs: List[MMDoc], metadatas: List[Dict], texts: List[str], use_faiss: bool = True):
        self.embeddings = embeddings
        self.docs = docs
        self.metadatas = metadatas
        self.texts = texts
        self.use_faiss = use_faiss and _HAS_FAISS
        if self.use_faiss:
            self.index = build_faiss_index(embeddings)
            print("âœ… Index FAISS crÃ©Ã©")
        else:
            self.index = None
            print("âœ… Index numpy crÃ©Ã©")

    def search(self, query_vec: np.ndarray, k: int = 5) -> List[Tuple[MMDoc, float]]:
        """Recherche les k documents les plus similaires"""
        query_vec = query_vec.reshape(1, -1).astype(np.float32)
        if self.use_faiss:
            scores, idxs = self.index.search(query_vec, k)
            scores = scores.flatten().tolist()
            idxs = idxs.flatten().tolist()
        else:
            sims = cosine_similarity(query_vec, self.embeddings).flatten()
            idxs = np.argsort(-sims)[:k]
            scores = sims[idxs].tolist()

        results = []
        for idx, score in zip(idxs, scores):
            results.append((self.docs[idx], float(score)))
        return results

def retrieve_and_build_prompt(query: str, store: SimpleVectorStore, image_store: Dict[str, str], k: int = 6) -> Tuple[str, List[MMDoc]]:
    """Recherche et construit le prompt"""
    qvec = embed_text_clip(query)
    qvec = qvec / (np.linalg.norm(qvec) + 1e-12)
    raw = store.search(qvec, k=k)

    docs = [r[0] for r in raw]

    # Construction du prompt
    prompt_parts = [
        "Vous Ãªtes un assistant utile. RÃ©pondez Ã  la question en utilisant uniquement le contexte fourni.",
        f"Question: {query}",
        "\nContexte:"
    ]

    for d in docs:
        m = d.metadata
        if m.get("type") == "text":
            prompt_parts.append(f"[Page {m.get('page')+1}] {d.text}")
        else:
            prompt_parts.append(f"[Page {m.get('page')+1}] [Image: {m.get('image_id')}]")

    prompt_parts.append("\nRÃ©pondez en franÃ§ais de maniÃ¨re concise et prÃ©cise en vous basant sur le contexte:")
    prompt = "\n".join(prompt_parts)

    return prompt, docs

# Cellule 10: Pipeline Principal avec BART
def create_multimodal_rag(pdf_path: str, rebuild_embeddings: bool = False) -> Dict:
    """Pipeline complet RAG multimodal avec BART"""
    print("ğŸ“¥ Extraction du PDF...")
    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    mm_docs, image_store = extract_pdf_multimodal(pdf_path, splitter=splitter)

    print(f"ğŸ“„ {len(mm_docs)} chunks multimodaux extraits (texte + images)")

    print("ğŸ”¨ Calcul des embeddings...")
    embeddings, metadatas, texts = build_embeddings(mm_docs, image_store, cache_path=EMBED_CACHE, force_recompute=rebuild_embeddings)

    print("ğŸª CrÃ©ation du vector store...")
    store = SimpleVectorStore(embeddings, mm_docs, metadatas, texts, use_faiss=True)

    print("ğŸ¤– Initialisation du LLM BART...")
    # UTILISATION DE BART QUI FONCTIONNE
    llm = HuggingFaceLLM(model_name="facebook/bart-large-cnn")

    return {"store": store, "image_store": image_store, "llm": llm, "docs": mm_docs}

def answer_query(query: str, env: Dict, k: int = 6):
    """RÃ©pond Ã  une question avec le RAG"""
    store = env["store"]
    image_store = env["image_store"]
    llm = env["llm"]

    prompt, retrieved_docs = retrieve_and_build_prompt(query, store, image_store, k=k)

    print("ğŸ¤– GÃ©nÃ©ration de la rÃ©ponse avec BART...")

    human_msg = HumanMessage(content=prompt)
    resp = llm.invoke([human_msg])

    # Extraction de la rÃ©ponse
    try:
        text = resp.content
    except Exception:
        try:
            text = str(resp)
        except Exception:
            text = "(aucun texte retournÃ©)"

    # RÃ©sumÃ© des documents rÃ©cupÃ©rÃ©s
    retrieved_summary = [{
        "id": d.id,
        "metadata": d.metadata,
        "preview": (d.text[:200] + "...") if len(d.text) > 200 else d.text
    } for d in retrieved_docs]

    return {"answer": text, "retrieved": retrieved_summary}

print("âœ… Pipeline RAG avec BART dÃ©fini!")

# Cellule 11: Interface Utilisateur
class MultimodalRAGSystem:
    def __init__(self):
        self.env = None
        self.pdf_path = None

    def load_pdf(self, pdf_path: str):
        """Charge un PDF dans le systÃ¨me RAG"""
        if not os.path.exists(pdf_path):
            print(f"âŒ Fichier {pdf_path} non trouvÃ©!")
            return False

        self.pdf_path = pdf_path
        print(f"ğŸ“– Chargement de: {pdf_path}")
        self.env = create_multimodal_rag(pdf_path, rebuild_embeddings=False)
        print("âœ… SystÃ¨me RAG avec BART prÃªt!")
        return True

    def ask_question(self, question: str, k: int = 5):
        """Pose une question au systÃ¨me"""
        if self.env is None:
            print("âŒ Veuillez d'abord charger un PDF avec load_pdf()")
            return None

        print(f"\nâ“ Question: {question}")
        result = answer_query(question, self.env, k=k)

        print(f"\nğŸ¤– RÃ©ponse BART:")
        print("â”€" * 60)
        print(result["answer"])
        print("â”€" * 60)

        print(f"\nğŸ” Documents retrouvÃ©s ({len(result['retrieved'])}):")
        for i, doc in enumerate(result["retrieved"]):
            doc_type = "ğŸ“ Texte" if doc["metadata"]["type"] == "text" else "ğŸ–¼ï¸ Image"
            print(f"  {i+1}. {doc_type} - Page {doc['metadata']['page']+1} - {doc['preview']}")

        return result

# CrÃ©ation de l'instance
rag_system = MultimodalRAGSystem()
print("âœ… SystÃ¨me RAG multimodal avec BART initialisÃ©!")

# Cellule 12: CHARGEMENT DU PDF - IMPORTANT!
# â­ REMPLACEZ PAR LE CHEMIN VERS VOTRE PDF â­

pdf_path = "votre_document.pdf"  # â† MODIFIEZ ICI !

# Si vous n'avez pas de PDF, crÃ©ez un fichier de test
if not os.path.exists(pdf_path):
    print("ğŸ“ CrÃ©ation d'un PDF de test...")
    # CrÃ©ation d'un simple PDF de test
    import tempfile
    pdf_path = "test_document.pdf"

    # Vous pouvez aussi uploader un fichier dans Colab
    try:
        from google.colab import files
        print("ğŸ“¤ Uploader votre PDF:")
        uploaded = files.upload()
        if uploaded:
            pdf_path = list(uploaded.keys())[0]
            print(f"âœ… PDF uploadÃ©: {pdf_path}")
    except:
        print("ğŸ’¡ Mettez votre PDF dans le mÃªme dossier et modifiez 'pdf_path'")

# Chargement du PDF
if os.path.exists(pdf_path):
    success = rag_system.load_pdf(pdf_path)
    if success:
        print("\nğŸ¯ Test avec une question exemple:")
        rag_system.ask_question("Quelle est la thÃ©matique principale de ce document?")
else:
    print(f"âŒ Fichier {pdf_path} non trouvÃ©!")
    print("ğŸ’¡ Veuillez mettre votre PDF dans le dossier ou modifier le chemin")

# Cellule 13: MODE INTERACTIF
def interactive_chat():
    """Mode conversationnel avec BART"""
    if rag_system.env is None:
        print("âŒ Veuillez d'abord charger un PDF dans la cellule prÃ©cÃ©dente!")
        return

    print("\n" + "="*70)
    print("ğŸ’¬ MODE INTERACTIF RAG MULTIMODAL - BART")
    print("="*70)
    print("Tapez 'quit' pour quitter, 'reset' pour changer de PDF")
    print("Le systÃ¨me utilise BART pour gÃ©nÃ©rer les rÃ©ponses")

    while True:
        question = input("\nâ“ Votre question: ").strip()

        if question.lower() in ['quit', 'exit', 'q']:
            print("ğŸ‘‹ Au revoir!")
            break
        elif question.lower() in ['reset', 'change']:
            new_pdf = input("ğŸ“ Nouveau chemin PDF: ").strip()
            rag_system.load_pdf(new_pdf)
            continue
        elif not question:
            continue

        # Pose la question au systÃ¨me
        rag_system.ask_question(question)

# Lancement du mode interactif
print("ğŸš€ Lancement du mode interactif...")
interactive_chat()

